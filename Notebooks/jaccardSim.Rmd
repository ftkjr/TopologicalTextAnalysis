---
title: "Mapping with Jaccard Similarity"
subtitle: "A First Approach"
author: "by Fred"
output:
  pdf_document: default
  html_notebook: default
---

```{r setup, include=FALSE}
##### Chunk Options ####
library(knitr)
opts_chunk$set(   echo = F,
               include = F,
               message = F,
               warning = F)
```

\section{Some Thoughts}
For the disinterested reader
\begin{enumerate}
  \item Notes on the connections in the set
    \begin{itemize}
      \item 
      Some sentences are related because of what seems like stop words alone.
    \end{itemize}
  
  \item Misspellings
    \begin{itemize}
      \item 
      Characters speech patterns are reflected in things like misspellings,
      overuse of particular words, etc.
    \end{itemize}
    
  \item Filtration!
  \begin{itemize}
  \item tfidf?
  \end{itemize}
  
\end{enumerate}

```{r packages}
##### Packages ####
# The usual suspects
library(readr)
library(magrittr)
library(tidyverse)

# Text analysis
library(text2vec)
library(stringr)

# Our very own
library(FredsVietorisRips)

# Graph adjacencies
library(igraph)

library(pander)
```


```{r import_data}
##### Import Data ####
df_original <- read_csv("D:/southparklines/All-seasons.csv")
```

```{r prep_text}
##### Prep text input ####
prep_fun <- function(x) {
    # make text lower case
    x = str_to_lower(x)
    # remove non-alphanumeric symbols
    x = str_replace_all(x, "[^[:alnum:]]", " ")
    # collapse multiple spaces
    str_replace_all(x, "\\s+", " ")
}
```


```{r sample_set}
##### Sample Dataset ####
# We need the full cleaned set later, 
# otherwise we wouldnt separate it out
df <- df_original %>%
  mutate(Line = prep_fun(Line))

# Take Sample
set.seed(1991)
df_sample <- df %>%
  sample_n(50)
```



```{r tokenize}
##### Tokenize Sample ####
it_sample <- df_sample$Line %>%
  itoken(progressbar = FALSE)
```

```{r}
vectorizer <- df$Line %>%
  itoken(progressbar = FALSE) %>%
  create_vocabulary() %>%
  prune_vocabulary(doc_proportion_max = 0.15) %>% 
  vocab_vectorizer()
```


\section{Jaccard Simliarity}

```{r doc_term_mat}
##### Document Term Matrix ####
dtm_sample <- create_dtm(it_sample, vectorizer)
dim(dtm_sample)

##### Jaccard Similarity ####
jacsim <- sim2(dtm_sample, dtm_sample, method = "jaccard", norm = "none")
dim(jacsim)


jsmat <- jacsim %>%
  as.matrix() 

jsmat <- 1 - jsmat 


adjmat <- jsmat %>%
  AdjacencyMatrix(epsilon = 0.95)
```

```{r plot_adjmat, include=TRUE}
graph_from_adjacency_matrix(adjmat, mode = "undirected") %>%
  plot()
``` 


```{r disconnected_lines, include=TRUE}
# df_sample$Line[rowSums(adjmat) < 2] %>%
  # kable(caption = "Lines with less than 2 adjacencies", col.names = NULL)

```

```{r connected_lines, include=TRUE}
# df_sample$Line[rowSums(adjmat) > max(rowSums(adjmat)) - 5] %>%
#   kable(caption = "Lines with high connections", col.names = NULL)
```


```{r all_the_lines, include=TRUE}
df_sample %>%
  mutate(Index = row_number()) %>%
  select(Index, Season, Episode, Character, Line) %>%
  pander(split.cell = 40, split.table = Inf)
```



```{r}
# MakeVectorizer <- function(lns) {
#   vectorizer <- lns %>%
#     itoken(progressbar = F) %>%
#     create_vocabulary() %>%
#     prune_vocabulary(doc_proportion_max = 0.1) %>%
#     vocab_vectorizer()
#   
#   return(vectorizer)
# }

```
















