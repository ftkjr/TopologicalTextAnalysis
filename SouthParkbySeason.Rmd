---
title: "South Park Season Similarity"
author: "Fred Kaesmann"
date: "6/22/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
     echo = TRUE,
  message = FALSE,
  warning = FALSE)
```

\section{Step 1: Collect Underpants}

We need the following packages and data:

```{r packages, warning=F, message=F}
##### Packages ####
# The usual suspects
library(readr)
library(magrittr)
library(tidyverse)

# Text analysis
library(text2vec)
library(stringr)
library(tidytext)
data("stop_words")
library(SnowballC)
library(wordcloud)

# Our very own
library(FredsVietorisRips)

# Graph adjacencies
library(igraph)
```

```{r import_data, message=FALSE, warning=FALSE}
##### Import Data ####
df_original <- read_csv("D:/southpark.csv")
```

Then the data needs to be cleaned and processed.
The cleaning process is as follows:
\begin{enumerate}
  \item Unnest the lines by word
  \item Remove stop words
  \item Group words by season
  \item Reassemble the words in the season into one entry
\end{enumerate}

```{r clean_data}
##### Clean Data ####
df <- df_original %>%
  unnest_tokens(word, Line) %>%
  anti_join(stop_words) %>%
  group_by(Season) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()

# Remove the last entry, because it's empty
df <- df[-nrow(df), ]
```


\section{Step 2: ???}

I don't really know what a lot of this does, I just used a popular R package

```{r tokenize}
##### Tokenize Sample ####
it_sample <- df$text %>%
  itoken(progressbar = FALSE)
```

```{r vectorizer}
##### Vectorize (?) ####
vectorizer <- df$text %>%
  itoken(progressbar = FALSE) %>%
  create_vocabulary() %>%
  prune_vocabulary() %>% 
  vocab_vectorizer()
```


This does some more stuff I'm not exactly sure of, then it runs the Jaccard
similarity.
After we get the Jaccard similarity, we subract it from 1 so that two bodies
of text which are comprised of the same words has a distance of 0.
```{r doc_term_mat}
##### Document Term Matrix ####
dtm_sample <- create_dtm(it_sample, vectorizer)
# dim(dtm_sample)

##### Jaccard Similarity ####
jacsim <- sim2(dtm_sample, dtm_sample, method = "jaccard", norm = "none")
# dim(jacsim)

##### Convert fancy object to matrix ####
jsmat <- jacsim %>%
  as.matrix() 

# Flip it around so 1 is max distance 
# and 0 indicates two sets are the same
jsmat <- 1 - jsmat 

```


\section{Step 3: Profit}

We plot increasing epsilon values


```{r}
for (ep in c(.735, .74, .745, .75, .756)) {
  ##### Plot Adjacencies ####
  jsmat %>%
    AdjacencyMatrix(ep) %>%
    graph_from_adjacency_matrix(mode = "undirected") %>%
    plot(main=paste("Epsilon =", ep))
}
```


We isloate the epsilon = 0.74 chart to examine the cluster centered on season 9.


```{r}
##### Create adjacency matrix ####
ep <- 0.74
adjmat <- jsmat %>%
    AdjacencyMatrix(epsilon = ep) 

##### Plot adjacencies ####
adjmat %>%
    graph_from_adjacency_matrix(mode = "undirected") %>%
    plot(main=paste("Epsilon =", ep))
```


```{r }
##### Isolate Adjacencies ####
connpoints <- which(adjmat[,9] == 1)
```


```{r evaluate_by_episode}
# Everything below was copied and pasted from above
##### Clean Data ####
df <- df_original %>%
  filter(Season == connpoints) %>%
  unnest_tokens(word, Line) %>%
  anti_join(stop_words) %>%
  group_by(Season, Episode) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()

# Remove the last entry, because it's empty
df <- df[-nrow(df), ]

##### Tokenize Sample ####
it_sample <- df$text %>%
  itoken(progressbar = FALSE)

##### Vectorize (?) ####
vectorizer <- df$text %>%
  itoken(progressbar = FALSE) %>%
  create_vocabulary() %>%
  prune_vocabulary() %>% 
  vocab_vectorizer()

##### Document Term Matrix ####
dtm_sample <- create_dtm(it_sample, vectorizer)
# dim(dtm_sample)

##### Jaccard Similarity ####
jacsim <- sim2(dtm_sample, dtm_sample, method = "jaccard", norm = "none")
# dim(jacsim)

##### Convert fancy object to matrix ####
jsmat <- jacsim %>%
  as.matrix() 

# Flip it around so 1 is max distance 
# and 0 indicates two sets are the same
jsmat <- 1 - jsmat 

```

```{r}
for (ep in c(.875, .885, .895, .9, .95)) {
  ##### Plot Adjacencies ####
  jsmat %>%
    AdjacencyMatrix(ep) %>%
    graph_from_adjacency_matrix(mode = "undirected") %>%
    plot(main=paste("Epsilon =", ep))
}
```

Similar to what we did before we select the epsilon = 0.895 adjacency map.
This time we want to select the cluster of points around 42 with the addition 
of point 17.

Here we look at the simiarity between the lines of each character.
```{r}
cluster <- c(17, 14, 42, 47, 13)

# In case of reset
df <- df_original %>%
  filter(Season == connpoints) %>%
  unnest_tokens(word, Line) %>%
  anti_join(stop_words) %>%
  group_by(Season, Episode) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()

df2 <- df_original %>%
  right_join(df[cluster, c(1,2)], by = c("Season", "Episode"))  %>%
  group_by(Season, Episode, Character) %>%
  add_count(name = "n_lines")
  
  df2 %>%
    mutate(SE = paste0(Season, Episode)) %>%
    group_by(SE) %>%
    filter(n_lines > 10) %>%
    ggplot(aes(x = reorder(Character, n_lines),
               y = n_lines)) + 
    geom_col() +
    xlab(NULL) + 
    ylab(NULL) +
    coord_flip() +
    facet_wrap(~SE)
```


```{r evaluate_by_episode}
# Everything below was copied and pasted from above
##### Clean Data ####
df <- df2 %>%
  unnest_tokens(word, Line) %>%
  anti_join(stop_words) %>%
  group_by(Season, Episode, Character) %>%
  summarise(text = str_c(word, collapse = " ")) %>% 
  ungroup()

# Remove the last entry, because it's empty
df <- df[-nrow(df), ]

##### Tokenize Sample ####
it_sample <- df$text %>%
  itoken(progressbar = FALSE)

##### Vectorize (?) ####
vectorizer <- df$text %>%
  itoken(progressbar = FALSE) %>%
  create_vocabulary() %>%
  prune_vocabulary() %>% 
  vocab_vectorizer()

##### Document Term Matrix ####
dtm_sample <- create_dtm(it_sample, vectorizer)
# dim(dtm_sample)

##### Jaccard Similarity ####
jacsim <- sim2(dtm_sample, dtm_sample, method = "jaccard", norm = "none")
# dim(jacsim)

##### Convert fancy object to matrix ####
jsmat <- jacsim %>%
  as.matrix() 

# Flip it around so 1 is max distance 
# and 0 indicates two sets are the same
jsmat <- 1 - jsmat 

```



```{r}
for (ep in c(.825, .835, .845, .855, .865)) {
  ##### Plot Adjacencies ####
  jsmat %>%
    AdjacencyMatrix(ep) %>%
    graph_from_adjacency_matrix(mode = "undirected") %>%
    plot(main=paste("Epsilon =", ep))
}
```












